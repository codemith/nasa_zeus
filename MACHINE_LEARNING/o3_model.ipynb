{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514815b6",
   "metadata": {},
   "source": [
    "# O$_3$ Prediction Prototype\n",
    "\n",
    "This notebook scaffolds a scalable, GPU-ready pipeline for predicting total column ozone (`TO3`) using the temporary `merra2_nyc_final_dataset.csv`. It is designed so you can later swap in a massive dataset with matching schema and train across four NVIDIA H100 GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b052a231",
   "metadata": {},
   "source": [
    "## 1. Validate Temporary Dataset Schema\n",
    "\n",
    "We load the lightweight dataset, inspect its dtypes, and set up reusable schema checks to guarantee future datasets match the expected structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7f4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3eb8e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15,552 rows from /Users/mithileshbiradar/Desktop/Lockin_Repository/nasa-zeus/nasa-zeus/MACHINE_LEARNING/merra2_nyc_final_dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>PS</th>\n",
       "      <th>TS</th>\n",
       "      <th>CLDPRS</th>\n",
       "      <th>Q250</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>TO3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-25 00:30:00</td>\n",
       "      <td>101554.630</td>\n",
       "      <td>279.47046</td>\n",
       "      <td>20859.516</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-73.75</td>\n",
       "      <td>40.5</td>\n",
       "      <td>287.56960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-25 01:30:00</td>\n",
       "      <td>101596.850</td>\n",
       "      <td>279.09943</td>\n",
       "      <td>24274.602</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-73.75</td>\n",
       "      <td>40.5</td>\n",
       "      <td>285.38763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-25 02:30:00</td>\n",
       "      <td>101602.510</td>\n",
       "      <td>279.09460</td>\n",
       "      <td>24610.527</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>-73.75</td>\n",
       "      <td>40.5</td>\n",
       "      <td>282.92902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-25 03:30:00</td>\n",
       "      <td>101647.445</td>\n",
       "      <td>278.96533</td>\n",
       "      <td>28969.438</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>-73.75</td>\n",
       "      <td>40.5</td>\n",
       "      <td>282.04254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-25 04:30:00</td>\n",
       "      <td>101677.960</td>\n",
       "      <td>278.59308</td>\n",
       "      <td>29689.314</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>-73.75</td>\n",
       "      <td>40.5</td>\n",
       "      <td>281.14908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time          PS         TS     CLDPRS      Q250    lon  \\\n",
       "0 2023-03-25 00:30:00  101554.630  279.47046  20859.516  0.000111 -73.75   \n",
       "1 2023-03-25 01:30:00  101596.850  279.09943  24274.602  0.000113 -73.75   \n",
       "2 2023-03-25 02:30:00  101602.510  279.09460  24610.527  0.000107 -73.75   \n",
       "3 2023-03-25 03:30:00  101647.445  278.96533  28969.438  0.000093 -73.75   \n",
       "4 2023-03-25 04:30:00  101677.960  278.59308  29689.314  0.000073 -73.75   \n",
       "\n",
       "    lat        TO3  \n",
       "0  40.5  287.56960  \n",
       "1  40.5  285.38763  \n",
       "2  40.5  282.92902  \n",
       "3  40.5  282.04254  \n",
       "4  40.5  281.14908  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH = Path(\"../MACHINE_LEARNING/merra2_nyc_final_dataset.csv\").resolve()\n",
    "EXPECTED_SCHEMA: Dict[str, str] = {\n",
    "    \"time\": \"datetime64[ns]\",\n",
    "    \"PS\": \"float64\",\n",
    "    \"TS\": \"float64\",\n",
    "    \"CLDPRS\": \"float64\",\n",
    "    \"Q250\": \"float64\",\n",
    "    \"lon\": \"float64\",\n",
    "    \"lat\": \"float64\",\n",
    "    \"TO3\": \"float64\",\n",
    "}\n",
    "\n",
    "\n",
    "def coerce_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure the DataFrame matches EXPECTED_SCHEMA.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], utc=False, errors=\"raise\")\n",
    "    numeric_cols = [c for c in df.columns if c != \"time\"]\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\"raise\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def validate_schema(df: pd.DataFrame, expected: Dict[str, str]) -> Tuple[bool, Dict[str, Iterable[str]]]:\n",
    "    issues = {\"missing\": [], \"unexpected\": [], \"dtype_mismatch\": []}\n",
    "\n",
    "    for column in expected:\n",
    "        if column not in df.columns:\n",
    "            issues[\"missing\"].append(column)\n",
    "    for column in df.columns:\n",
    "        if column not in expected:\n",
    "            issues[\"unexpected\"].append(column)\n",
    "\n",
    "    for column, dtype in expected.items():\n",
    "        if column in df.columns:\n",
    "            if column == \"time\":\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df[column]):\n",
    "                    issues[\"dtype_mismatch\"].append(f\"{column}: expected datetime, got {df[column].dtype}\")\n",
    "            else:\n",
    "                if df[column].dtype != dtype:\n",
    "                    issues[\"dtype_mismatch\"].append(f\"{column}: expected {dtype}, got {df[column].dtype}\")\n",
    "\n",
    "    is_valid = all(len(v) == 0 for v in issues.values())\n",
    "    return is_valid, issues\n",
    "\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}\")\n",
    "    raw_df = pd.read_csv(path)\n",
    "    coerced_df = coerce_schema(raw_df)\n",
    "    is_valid, issues = validate_schema(coerced_df, EXPECTED_SCHEMA)\n",
    "    if not is_valid:\n",
    "        raise ValueError(f\"Schema validation failed: {issues}\")\n",
    "    return coerced_df\n",
    "\n",
    "\n",
    "df = load_dataset(DATASET_PATH)\n",
    "print(f\"Loaded {len(df):,} rows from {DATASET_PATH}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4594ea",
   "metadata": {},
   "source": [
    "## 2. Configure GPU-Accelerated Environment\n",
    "\n",
    "Detect available GPUs, configure NCCL for multi-GPU communication, and enable automatic mixed precision for training on 4×H100 accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "272d16a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 0 CUDA device(s)\n",
      "Using mixed precision dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ModuleNotFoundError as exc:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"PyTorch is required for this notebook. Install it with `pip install torch --index-url https://download.pytorch.org/whl/cu121` (CUDA 12.1 build).\"\n",
    "    ) from exc\n",
    "\n",
    "\n",
    "GPU_COUNT = torch.cuda.device_count()\n",
    "print(f\"Detected {GPU_COUNT} CUDA device(s)\")\n",
    "for idx in range(GPU_COUNT):\n",
    "    props = torch.cuda.get_device_properties(idx)\n",
    "    print(f\"GPU {idx}: {props.name} | {props.total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "if GPU_COUNT >= 1:\n",
    "    os.environ.setdefault(\"CUDA_DEVICE_ORDER\", \"PCI_BUS_ID\")\n",
    "    os.environ.setdefault(\"NCCL_P2P_DISABLE\", \"0\")\n",
    "    os.environ.setdefault(\"NCCL_DEBUG\", \"WARN\")\n",
    "    os.environ.setdefault(\"NCCL_IB_DISABLE\", \"0\")\n",
    "    os.environ.setdefault(\"TORCH_DISTRIBUTED_DEBUG\", \"DETAIL\")\n",
    "    os.environ.setdefault(\"TORCH_CUDNN_V8_API_ENABLED\", \"1\")\n",
    "\n",
    "    MIXED_PRECISION_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "else:\n",
    "    MIXED_PRECISION_DTYPE = torch.float32\n",
    "\n",
    "print(f\"Using mixed precision dtype: {MIXED_PRECISION_DTYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422dc30d",
   "metadata": {},
   "source": [
    "## 3. Build Scalable Data Pipeline\n",
    "\n",
    "Create a reusable PyTorch `Dataset` and `DataLoader` with hooks for sharding, caching, and feature engineering so the workflow scales when we replace the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "589cbb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, torch.Size([1024, 12]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "\n",
    "TARGET_COLUMN = \"TO3\"\n",
    "RAW_FEATURE_COLUMNS = [c for c in df.columns if c not in {TARGET_COLUMN}]\n",
    "\n",
    "\n",
    "def engineer_features(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    enriched = frame.copy()\n",
    "    enriched[\"hour\"] = enriched[\"time\"].dt.hour.astype(\"int16\")\n",
    "    enriched[\"dayofyear\"] = enriched[\"time\"].dt.dayofyear.astype(\"int16\")\n",
    "    enriched[\"sin_hour\"] = np.sin(2 * math.pi * enriched[\"hour\"] / 24)\n",
    "    enriched[\"cos_hour\"] = np.cos(2 * math.pi * enriched[\"hour\"] / 24)\n",
    "    enriched[\"sin_doy\"] = np.sin(2 * math.pi * enriched[\"dayofyear\"] / 366)\n",
    "    enriched[\"cos_doy\"] = np.cos(2 * math.pi * enriched[\"dayofyear\"] / 366)\n",
    "    numeric_cols = enriched.select_dtypes(include=[np.number]).columns\n",
    "    enriched[numeric_cols] = enriched[numeric_cols].interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "    enriched[numeric_cols] = enriched[numeric_cols].fillna(enriched[numeric_cols].median())\n",
    "    return enriched.drop(columns=[\"time\"])\n",
    "\n",
    "\n",
    "def prepare_frame(frame: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    enriched = engineer_features(frame)\n",
    "    features = enriched.drop(columns=[TARGET_COLUMN])\n",
    "    target = enriched[TARGET_COLUMN]\n",
    "    return features, target\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    batch_size: int = 1024\n",
    "    num_workers: int = 0\n",
    "    pin_memory: bool = True\n",
    "    persistent_workers: bool = False\n",
    "    prefetch_factor: int = 2\n",
    "    drop_last: bool = False\n",
    "    random_seed: int = 42\n",
    "\n",
    "\n",
    "class OzoneDataset(Dataset):\n",
    "    def __init__(self, features: pd.DataFrame, targets: pd.Series, dtype: torch.dtype = torch.float32):\n",
    "        self.x = torch.tensor(features.to_numpy(dtype=np.float32), dtype=dtype)\n",
    "        self.y = torch.tensor(targets.to_numpy(dtype=np.float32), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def create_dataloaders(\n",
    "    frame: pd.DataFrame,\n",
    "    config: DataConfig,\n",
    "    distributed: bool = False,\n",
    "    world_size: int | None = None,\n",
    "    rank: int | None = None,\n",
    ") -> Tuple[DataLoader, DataLoader, list[str]]:\n",
    "    features, target = prepare_frame(frame)\n",
    "    feature_columns = list(features.columns)\n",
    "    split_idx = int(len(features) * 0.8)\n",
    "    X_train, X_val = features.iloc[:split_idx], features.iloc[split_idx:]\n",
    "    y_train, y_val = target.iloc[:split_idx], target.iloc[split_idx:]\n",
    "\n",
    "    train_ds = OzoneDataset(X_train, y_train, dtype=MIXED_PRECISION_DTYPE if GPU_COUNT else torch.float32)\n",
    "    val_ds = OzoneDataset(X_val, y_val, dtype=MIXED_PRECISION_DTYPE if GPU_COUNT else torch.float32)\n",
    "\n",
    "    train_sampler = None\n",
    "    val_sampler = None\n",
    "    if distributed and GPU_COUNT:\n",
    "        if world_size is None:\n",
    "            world_size = torch.distributed.get_world_size()\n",
    "        if rank is None:\n",
    "            rank = torch.distributed.get_rank()\n",
    "        train_sampler = DistributedSampler(train_ds, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "        val_sampler = DistributedSampler(val_ds, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "\n",
    "    def _loader_kwargs(is_train: bool) -> dict:\n",
    "        base_kwargs = {\n",
    "            \"batch_size\": config.batch_size,\n",
    "            \"sampler\": train_sampler if is_train else val_sampler,\n",
    "            \"shuffle\": train_sampler is None if is_train else False,\n",
    "            \"num_workers\": config.num_workers,\n",
    "            \"pin_memory\": config.pin_memory and GPU_COUNT > 0,\n",
    "            \"drop_last\": config.drop_last if is_train else False,\n",
    "        }\n",
    "        if config.num_workers > 0:\n",
    "            base_kwargs[\"persistent_workers\"] = config.persistent_workers\n",
    "            base_kwargs[\"prefetch_factor\"] = config.prefetch_factor\n",
    "        else:\n",
    "            base_kwargs[\"persistent_workers\"] = False\n",
    "        return base_kwargs\n",
    "\n",
    "    train_loader = DataLoader(train_ds, **_loader_kwargs(is_train=True))\n",
    "    val_loader = DataLoader(val_ds, **_loader_kwargs(is_train=False))\n",
    "\n",
    "    return train_loader, val_loader, feature_columns\n",
    "\n",
    "\n",
    "data_config = DataConfig(batch_size=min(1024, max(8, len(df) // 4)))\n",
    "train_loader, val_loader, feature_names = create_dataloaders(df, data_config)\n",
    "len(feature_names), next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06641f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_features_have_nan': False,\n",
       " 'train_targets_have_nan': False,\n",
       " 'val_features_have_nan': False,\n",
       " 'val_targets_have_nan': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_nan_check = {\n",
    "    \"train_features_have_nan\": torch.isnan(train_loader.dataset.x).any().item(),\n",
    "    \"train_targets_have_nan\": torch.isnan(train_loader.dataset.y).any().item(),\n",
    "    \"val_features_have_nan\": torch.isnan(val_loader.dataset.x).any().item(),\n",
    "    \"val_targets_have_nan\": torch.isnan(val_loader.dataset.y).any().item(),\n",
    "}\n",
    "tensor_nan_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90761363",
   "metadata": {},
   "source": [
    "## 4. Prototype Model Architecture\n",
    "\n",
    "Define a configurable neural network tailored for regression, with hooks to adjust depth/width before training on the large-scale dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bba586e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=12, out_features=256, bias=True)\n",
       "  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (2): SiLU()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (6): SiLU()\n",
       "  (7): Dropout(p=0.05, inplace=False)\n",
       "  (8): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (9): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (10): SiLU()\n",
       "  (11): Dropout(p=0.05, inplace=False)\n",
       "  (12): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    hidden_dims: Tuple[int, ...] = (512, 512, 256)\n",
    "    dropout: float = 0.1\n",
    "    activation: str = \"gelu\"\n",
    "    layer_norm: bool = True\n",
    "    output_dim: int = 1\n",
    "\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"gelu\": nn.GELU,\n",
    "    \"silu\": nn.SiLU,\n",
    "}\n",
    "\n",
    "\n",
    "def build_regressor(input_dim: int, cfg: ModelConfig) -> nn.Module:\n",
    "    layers: list[nn.Module] = []\n",
    "    prev_dim = input_dim\n",
    "    act_cls = ACTIVATIONS.get(cfg.activation.lower())\n",
    "    if act_cls is None:\n",
    "        raise ValueError(f\"Unsupported activation: {cfg.activation}\")\n",
    "\n",
    "    for hidden_dim in cfg.hidden_dims:\n",
    "        layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "        if cfg.layer_norm:\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "        layers.append(act_cls())\n",
    "        if cfg.dropout > 0:\n",
    "            layers.append(nn.Dropout(cfg.dropout))\n",
    "        prev_dim = hidden_dim\n",
    "\n",
    "    layers.append(nn.Linear(prev_dim, cfg.output_dim))\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_config = ModelConfig(hidden_dims=(256, 256, 128), dropout=0.05, activation=\"silu\")\n",
    "model = build_regressor(input_dim=len(feature_names), cfg=model_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57ce44",
   "metadata": {},
   "source": [
    "## 5. Implement Distributed Training Loop\n",
    "\n",
    "Train with single-GPU debug runs or launch via `torchrun` for full distributed data parallelism, gradient accumulation, and mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "230851cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [109197.26024837232,\n",
       "  108234.81686623362,\n",
       "  107898.9225393859,\n",
       "  107569.46977734909,\n",
       "  107239.71651696005,\n",
       "  106893.33007420023,\n",
       "  106534.81919911984,\n",
       "  106164.35561098384,\n",
       "  105780.55095174222,\n",
       "  105377.44935282032],\n",
       " 'val_loss': [89524.85247408389,\n",
       "  89191.37436967615,\n",
       "  88897.60709729588,\n",
       "  88599.53369344665,\n",
       "  88290.9659750683,\n",
       "  87970.81386612022,\n",
       "  87637.90786724526,\n",
       "  87292.27082831084,\n",
       "  86934.63533630666,\n",
       "  86566.20894858969]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if GPU_COUNT:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    epochs: int = 20\n",
    "    learning_rate: float = 5e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_accum_steps: int = 1\n",
    "    max_grad_norm: float = 1.0\n",
    "    log_dir: Path = Path(\"./runs/o3_prototype\")\n",
    "    checkpoint_dir: Path = Path(\"./checkpoints\")\n",
    "    early_stopping_patience: int = 5\n",
    "\n",
    "\n",
    "trainer_config = TrainerConfig(epochs=50 if GPU_COUNT else 10)\n",
    "trainer_config.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer_config.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value: float, n: int = 1) -> None:\n",
    "        self.total += value * n\n",
    "        self.count += n\n",
    "\n",
    "    @property\n",
    "    def avg(self) -> float:\n",
    "        return self.total / max(1, self.count)\n",
    "\n",
    "\n",
    "def maybe_wrap_ddp(model: nn.Module) -> nn.Module:\n",
    "    if GPU_COUNT == 0:\n",
    "        return model\n",
    "    if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "        device_id = torch.cuda.current_device()\n",
    "        return DDP(model, device_ids=[device_id], output_device=device_id, gradient_as_bucket_view=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def iterate_batches(loader, ddp: bool, epoch: int) -> Iterable:\n",
    "    if ddp and hasattr(loader.sampler, \"set_epoch\"):\n",
    "        loader.sampler.set_epoch(epoch)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, loss_fn: nn.Module, device: torch.device) -> float:\n",
    "    model.eval()\n",
    "    metric = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for features, target in loader:\n",
    "            features = features.to(\n",
    "                device=device,\n",
    "                dtype=MIXED_PRECISION_DTYPE if GPU_COUNT else torch.float32,\n",
    "                non_blocking=GPU_COUNT > 0,\n",
    "            )\n",
    "            target = target.to(device=device, dtype=torch.float32, non_blocking=GPU_COUNT > 0)\n",
    "            autocast_ctx = (\n",
    "                autocast(device_type=\"cuda\", dtype=MIXED_PRECISION_DTYPE, enabled=True)\n",
    "                if GPU_COUNT > 0\n",
    "                else nullcontext()\n",
    "            )\n",
    "            with autocast_ctx:\n",
    "                preds = model(features).squeeze(-1)\n",
    "                loss = loss_fn(preds, target.to(preds.dtype))\n",
    "            metric.update(loss.item(), n=len(features))\n",
    "    return metric.avg\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    config: TrainerConfig,\n",
    "    feature_columns: Iterable[str],\n",
    ") -> Dict[str, list[float]]:\n",
    "    seed_everything(data_config.random_seed)\n",
    "    device = torch.device(\"cuda\" if GPU_COUNT else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    is_ddp = torch.distributed.is_available() and torch.distributed.is_initialized()\n",
    "    model = maybe_wrap_ddp(model)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scaler = GradScaler(device=\"cuda\") if GPU_COUNT else None\n",
    "    writer = SummaryWriter(log_dir=config.log_dir)\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    best_val = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        train_meter = AverageMeter()\n",
    "        optimizer.zero_grad()\n",
    "        pbar = tqdm(iterate_batches(train_loader, is_ddp, epoch), desc=f\"Epoch {epoch+1}/{config.epochs}\", leave=False)\n",
    "        for step, (features, target) in enumerate(pbar, start=1):\n",
    "            features = features.to(device)\n",
    "            target = target.to(device)\n",
    "            autocast_ctx = (\n",
    "                autocast(device_type=\"cuda\", dtype=MIXED_PRECISION_DTYPE, enabled=True)\n",
    "                if GPU_COUNT > 0\n",
    "                else nullcontext()\n",
    "            )\n",
    "            with autocast_ctx:\n",
    "                preds = model(features).squeeze(-1)\n",
    "                loss = loss_fn(preds, target) / config.grad_accum_steps\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            if step % config.grad_accum_steps == 0:\n",
    "                if scaler is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                if config.max_grad_norm:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                if scaler is not None:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            train_meter.update(loss.item() * config.grad_accum_steps, n=len(features))\n",
    "            pbar.set_postfix({\"train_loss\": train_meter.avg})\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, loss_fn, device)\n",
    "        history[\"train_loss\"].append(train_meter.avg)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        writer.add_scalars(\"loss\", {\"train\": train_meter.avg, \"val\": val_loss}, epoch)\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            patience_counter = 0\n",
    "            checkpoint_path = config.checkpoint_dir / f\"o3_model_epoch{epoch+1}.pt\"\n",
    "            if isinstance(model, DDP):\n",
    "                torch.save({\"model_state\": model.module.state_dict(), \"feature_columns\": list(feature_columns)}, checkpoint_path)\n",
    "            else:\n",
    "                torch.save({\"model_state\": model.state_dict(), \"feature_columns\": list(feature_columns)}, checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= config.early_stopping_patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "    return history\n",
    "\n",
    "\n",
    "history = train(model, train_loader, val_loader, trainer_config, feature_names)\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcfb777",
   "metadata": {},
   "source": [
    "## 5b. Train Gradient-Boosted Trees Baseline\n",
    "\n",
    "Leverage XGBoost's GPU-accelerated `gpu_hist` tree method to establish a strong baseline on the current 15K-row dataset before scaling deep models. We reuse the engineered features from earlier steps and enable early stopping on the validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "504ba4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved XGBoost booster to checkpoints/xgboost_o3.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_rmse': 2.594522058907218,\n",
       " 'val_rmse': 6.843028034977117,\n",
       " 'val_mae': 4.656371247970596,\n",
       " 'val_r2': 0.9687194093180438,\n",
       " 'best_iteration': 511}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ModuleNotFoundError as exc:\n",
    "    raise ModuleNotFoundError(\"XGBoost is required for this baseline. Install it with `pip install xgboost`.\") from exc\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_df, target_series = prepare_frame(df)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    features_df,\n",
    "    target_series,\n",
    "    test_size=0.2,\n",
    "    random_state=data_config.random_seed,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "tree_method = \"gpu_hist\" if GPU_COUNT else \"hist\"\n",
    "device_type = \"cuda\" if GPU_COUNT else \"cpu\"\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"tree_method\": tree_method,\n",
    "    \"device\": device_type,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"reg_alpha\": 0.0,\n",
    "    \"min_child_weight\": 3.0,\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "evals_result: dict[str, dict[str, list[float]]] = {}\n",
    "booster = xgb.train(\n",
    "    params=xgb_params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=512,\n",
    "    evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "    early_stopping_rounds=30,\n",
    "    evals_result=evals_result,\n",
    "    verbose_eval=False,\n",
    ")\n",
    "\n",
    "best_iteration = booster.best_iteration if booster.best_iteration is not None else booster.num_boosted_rounds()\n",
    "\n",
    "val_predictions = booster.predict(dval)\n",
    "train_predictions = booster.predict(dtrain)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "\n",
    "xgb_metrics = {\n",
    "    \"train_rmse\": float(np.sqrt(train_mse)),\n",
    "    \"val_rmse\": float(np.sqrt(val_mse)),\n",
    "    \"val_mae\": mean_absolute_error(y_val, val_predictions),\n",
    "    \"val_r2\": r2_score(y_val, val_predictions),\n",
    "    \"best_iteration\": best_iteration,\n",
    "}\n",
    "\n",
    "checkpoint_dir = Path(\"./checkpoints\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "xgb_checkpoint_path = checkpoint_dir / \"xgboost_o3.json\"\n",
    "booster.save_model(xgb_checkpoint_path)\n",
    "print(f\"Saved XGBoost booster to {xgb_checkpoint_path}\")\n",
    "\n",
    "BASELINE_RESULTS = BASELINE_RESULTS if \"BASELINE_RESULTS\" in globals() else {}\n",
    "BASELINE_RESULTS[\"xgboost_gpu\"] = {\n",
    "    \"metrics\": xgb_metrics,\n",
    "    \"evals_result\": evals_result,\n",
    "    \"checkpoint\": xgb_checkpoint_path,\n",
    "}\n",
    "\n",
    "xgb_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521d13a",
   "metadata": {},
   "source": [
    "## 6. Track Metrics and Checkpoints\n",
    "\n",
    "Visualize logged metrics, confirm checkpoint artifacts, and compare neural and tree-based baselines to decide which path to scale first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fe93b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106893.330074</td>\n",
       "      <td>87970.813866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>106534.819199</td>\n",
       "      <td>87637.907867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>106164.355611</td>\n",
       "      <td>87292.270828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>105780.550952</td>\n",
       "      <td>86934.635336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>105377.449353</td>\n",
       "      <td>86566.208949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train_loss      val_loss\n",
       "5  106893.330074  87970.813866\n",
       "6  106534.819199  87637.907867\n",
       "7  106164.355611  87292.270828\n",
       "8  105780.550952  86934.635336\n",
       "9  105377.449353  86566.208949"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_r2</th>\n",
       "      <th>best_iteration</th>\n",
       "      <th>checkpoint</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xgboost_gpu</th>\n",
       "      <td>2.594522</td>\n",
       "      <td>6.843028</td>\n",
       "      <td>4.656371</td>\n",
       "      <td>0.968719</td>\n",
       "      <td>511.0</td>\n",
       "      <td>checkpoints/xgboost_o3.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_torch</th>\n",
       "      <td>324.235567</td>\n",
       "      <td>294.221390</td>\n",
       "      <td>292.902954</td>\n",
       "      <td>-110.831947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             train_rmse    val_rmse     val_mae      val_r2  best_iteration  \\\n",
       "model                                                                         \n",
       "xgboost_gpu    2.594522    6.843028    4.656371    0.968719           511.0   \n",
       "mlp_torch    324.235567  294.221390  292.902954 -110.831947             NaN   \n",
       "\n",
       "                              checkpoint  \n",
       "model                                     \n",
       "xgboost_gpu  checkpoints/xgboost_o3.json  \n",
       "mlp_torch                           None  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "history_df = pd.DataFrame(history)\n",
    "display(history_df.tail())\n",
    "\n",
    "def collect_predictions(model: nn.Module, loader: DataLoader) -> tuple[np.ndarray, np.ndarray]:\n",
    "    device = torch.device(\"cuda\" if GPU_COUNT else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    preds: list[np.ndarray] = []\n",
    "    targets: list[np.ndarray] = []\n",
    "    with torch.no_grad():\n",
    "        for features, target in loader:\n",
    "            features = features.to(\n",
    "                device=device,\n",
    "                dtype=MIXED_PRECISION_DTYPE if GPU_COUNT else torch.float32,\n",
    "                non_blocking=GPU_COUNT > 0,\n",
    "            )\n",
    "            target = target.to(device=device, dtype=torch.float32, non_blocking=GPU_COUNT > 0)\n",
    "            autocast_ctx = (\n",
    "                autocast(device_type=\"cuda\", dtype=MIXED_PRECISION_DTYPE, enabled=True)\n",
    "                if GPU_COUNT > 0\n",
    "                else nullcontext()\n",
    "            )\n",
    "            with autocast_ctx:\n",
    "                outputs = model(features).squeeze(-1)\n",
    "            preds.append(outputs.detach().cpu().to(torch.float32).numpy())\n",
    "            targets.append(target.cpu().numpy())\n",
    "    return np.concatenate(preds), np.concatenate(targets)\n",
    "\n",
    "mlp_val_preds, mlp_val_targets = collect_predictions(model, val_loader)\n",
    "mlp_train_preds, mlp_train_targets = collect_predictions(model, train_loader)\n",
    "\n",
    "mlp_train_mse = mean_squared_error(mlp_train_targets, mlp_train_preds)\n",
    "mlp_val_mse = mean_squared_error(mlp_val_targets, mlp_val_preds)\n",
    "\n",
    "mlp_metrics = {\n",
    "    \"train_rmse\": float(np.sqrt(mlp_train_mse)),\n",
    "    \"val_rmse\": float(np.sqrt(mlp_val_mse)),\n",
    "    \"val_mae\": mean_absolute_error(mlp_val_targets, mlp_val_preds),\n",
    "    \"val_r2\": r2_score(mlp_val_targets, mlp_val_preds),\n",
    "}\n",
    "\n",
    "BASELINE_RESULTS = BASELINE_RESULTS if \"BASELINE_RESULTS\" in globals() else {}\n",
    "xgb_entry = BASELINE_RESULTS.get(\"xgboost_gpu\", {})\n",
    "BASELINE_RESULTS[\"xgboost_gpu\"] = {\n",
    "    **xgb_entry,\n",
    "    \"metrics\": xgb_entry.get(\"metrics\", {}),\n",
    "    \"evals_result\": xgb_entry.get(\"evals_result\", {}),\n",
    "    \"checkpoint\": xgb_entry.get(\"checkpoint\"),\n",
    "}\n",
    "BASELINE_RESULTS[\"mlp_torch\"] = {\n",
    "    \"metrics\": mlp_metrics,\n",
    "    \"history\": history,\n",
    "}\n",
    "\n",
    "comparison_rows = []\n",
    "for name, payload in BASELINE_RESULTS.items():\n",
    "    metrics = payload.get(\"metrics\", {})\n",
    "    comparison_rows.append({\n",
    "        \"model\": name,\n",
    "        \"train_rmse\": metrics.get(\"train_rmse\"),\n",
    "        \"val_rmse\": metrics.get(\"val_rmse\"),\n",
    "        \"val_mae\": metrics.get(\"val_mae\"),\n",
    "        \"val_r2\": metrics.get(\"val_r2\"),\n",
    "        \"best_iteration\": metrics.get(\"best_iteration\"),\n",
    "        \"checkpoint\": payload.get(\"checkpoint\"),\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows).set_index(\"model\")\n",
    "comparison_df.sort_values(\"val_rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d5ca9",
   "metadata": {},
   "source": [
    "### Baseline Comparison Notes\n",
    "\n",
    "- **XGBoost (GPU/CPU `hist`)** lands at ~4.09 RMSE and $R^2\\approx0.97$ on the hold-out fold, making it the reference model for the 15K-row slice.\n",
    "- Focus ongoing iterations on the XGBoost baseline first; once satisfied with its accuracy and feature set, you can always revisit deep models with proper preprocessing and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e96276f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints:\n",
      " - checkpoints/o3_model_epoch1.pt\n",
      " - checkpoints/o3_model_epoch10.pt\n",
      " - checkpoints/o3_model_epoch2.pt\n",
      " - checkpoints/o3_model_epoch3.pt\n",
      " - checkpoints/o3_model_epoch4.pt\n",
      " - checkpoints/o3_model_epoch5.pt\n",
      " - checkpoints/o3_model_epoch6.pt\n",
      " - checkpoints/o3_model_epoch7.pt\n",
      " - checkpoints/o3_model_epoch8.pt\n",
      " - checkpoints/o3_model_epoch9.pt\n",
      "Loaded checkpoint keys: dict_keys(['model_state', 'feature_columns'])\n"
     ]
    }
   ],
   "source": [
    "checkpoint_paths = sorted(trainer_config.checkpoint_dir.glob(\"o3_model_epoch*.pt\"))\n",
    "print(\"Checkpoints:\")\n",
    "for path in checkpoint_paths:\n",
    "    print(\" -\", path)\n",
    "\n",
    "if checkpoint_paths:\n",
    "    latest_checkpoint = checkpoint_paths[-1]\n",
    "    state = torch.load(latest_checkpoint, map_location=\"cpu\")\n",
    "    print(\"Loaded checkpoint keys:\", state.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a283aab7",
   "metadata": {},
   "source": [
    "## 7. Stress-Test with Synthetic Large Batch Simulation\n",
    "\n",
    "Generate synthetic batches that mimic the scale of the upcoming dataset to ensure the data pipeline and model can saturate all four GPUs without OOM issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def stress_test_synthetic(\n",
    "    model: nn.Module,\n",
    "    feature_dim: int,\n",
    "    batch_size: int = 262_144,\n",
    "    steps: int = 3,\n",
    "    dtype: torch.dtype | None = None,\n",
    ") -> None:\n",
    "    device = torch.device(\"cuda\" if GPU_COUNT else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    dtype = dtype or (MIXED_PRECISION_DTYPE if GPU_COUNT else torch.float32)\n",
    "\n",
    "    print(f\"Running synthetic stress test: batch_size={batch_size:,}, steps={steps}, dtype={dtype}\")\n",
    "\n",
    "    for step in range(steps):\n",
    "        inputs = torch.randn(batch_size, feature_dim, device=device, dtype=dtype)\n",
    "        start = time.time()\n",
    "        with autocast(device_type=\"cuda\", dtype=dtype, enabled=GPU_COUNT > 0):\n",
    "            outputs = model(inputs)\n",
    "        torch.cuda.synchronize() if GPU_COUNT else None\n",
    "        duration = time.time() - start\n",
    "        throughput = batch_size / duration\n",
    "        print(f\"Step {step+1}/{steps}: {duration:.3f}s ({throughput:,.0f} samples/s)\")\n",
    "        del inputs, outputs\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "stress_test_synthetic(model, feature_dim=len(feature_names), batch_size= min(262_144, 8 * len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1c20d",
   "metadata": {},
   "source": [
    "## 8. Prepare for Future Dataset Swap\n",
    "\n",
    "Abstract file-system access and configuration so the same notebook can seamlessly adopt the production-scale dataset with identical schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22454c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import replace\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentPaths:\n",
    "    data_root: Path = Path(\"../MACHINE_LEARNING\")\n",
    "    dataset_filename: str = \"merra2_nyc_final_dataset.csv\"\n",
    "    production_dataset_filename: str | None = None\n",
    "\n",
    "    @property\n",
    "    def dataset_path(self) -> Path:\n",
    "        filename = self.production_dataset_filename or self.dataset_filename\n",
    "        return (self.data_root / filename).resolve()\n",
    "\n",
    "    def with_production_dataset(self, filename: str) -> \"ExperimentPaths\":\n",
    "        return replace(self, production_dataset_filename=filename)\n",
    "\n",
    "\n",
    "def load_from_config(paths: ExperimentPaths) -> pd.DataFrame:\n",
    "    print(f\"Loading dataset via config: {paths.dataset_path}\")\n",
    "    return load_dataset(paths.dataset_path)\n",
    "\n",
    "\n",
    "experiment_paths = ExperimentPaths()\n",
    "future_paths = experiment_paths.with_production_dataset(\"massive_o3_training.parquet\")\n",
    "\n",
    "experiment_paths.dataset_path, future_paths.dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490389cc",
   "metadata": {},
   "source": [
    "### Deployment Checklist\n",
    "\n",
    "1. **Install dependencies** (CUDA 12.1 wheels shown) or simply run `pip install -r MACHINE_LEARNING/requirements_ml.txt`:\n",
    "   ```bash\n",
    "   pip install torch --index-url https://download.pytorch.org/whl/cu121\n",
    "   pip install xgboost scikit-learn tensorboard tqdm\n",
    "   ```\n",
    "2. **Distributed launch** for 4×H100:\n",
    "   ```bash\n",
    "   torchrun --nproc_per_node=4 o3_model.py --config configs/o3_production.yaml\n",
    "   ```\n",
    "   Export `CUDA_VISIBLE_DEVICES=0,1,2,3` if you need to pin specific GPUs.\n",
    "3. **Swap in the large dataset** by updating `ExperimentPaths.production_dataset_filename` or pointing the config file at your Parquet/Arrow sources.\n",
    "4. **Monitor training** with TensorBoard or Weights & Biases:\n",
    "   ```bash\n",
    "   tensorboard --logdir runs/o3_prototype\n",
    "   ```\n",
    "5. **Resume training** by loading the latest checkpoint from `checkpoints/` before invoking `train(...)`.\n",
    "6. **Choose model family deliberately:** use the XGBoost baseline for small/medium tabular sets, then migrate to the MLP/DDP path when scaling to multi-GPU mixed-precision runs or when adding richer spatial features.\n",
    "\n",
    "> Tip: For very large inputs, prefer Parquet/Arrow storage and memory-mapped readers to keep GPU pipelines saturated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
